{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Cognitive Skills & Performance Analysis\n",
    "\n",
    "This notebook demonstrates a comprehensive machine learning workflow for analyzing student cognitive skills and predicting academic performance. We'll use synthetic data to explore correlations, build predictive models, and cluster students into learning personas.\n",
    "\n",
    "## Dataset Structure\n",
    "- **student_id**: Unique identifier for each student\n",
    "- **name**: Student name\n",
    "- **class**: Academic class/grade level\n",
    "- **comprehension**: Reading and understanding ability (0-100)\n",
    "- **attention**: Ability to focus and concentrate (0-100)\n",
    "- **focus**: Sustained attention during tasks (0-100)\n",
    "- **retention**: Memory and information retention (0-100)\n",
    "- **assessment_score**: Academic performance score (0-100)\n",
    "- **engagement_time**: Weekly study engagement in hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Synthetic Dataset Generation\n",
    "\n",
    "We'll create a realistic synthetic dataset with 500 students, incorporating realistic correlations between cognitive skills and academic performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_student_data(n_students=500):\n",
    "    \"\"\"\n",
    "    Generate synthetic student data with realistic correlations between cognitive skills and performance.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate student IDs and names\n",
    "    student_ids = range(1, n_students + 1)\n",
    "    first_names = ['Alex', 'Jordan', 'Taylor', 'Casey', 'Morgan', 'Riley', 'Avery', 'Quinn', 'Sage', 'River',\n",
    "                   'Emma', 'Liam', 'Olivia', 'Noah', 'Ava', 'Ethan', 'Sophia', 'Mason', 'Isabella', 'William',\n",
    "                   'Mia', 'James', 'Charlotte', 'Benjamin', 'Amelia', 'Lucas', 'Harper', 'Henry', 'Evelyn', 'Alexander']\n",
    "    last_names = ['Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Garcia', 'Miller', 'Davis', 'Rodriguez', 'Martinez',\n",
    "                  'Hernandez', 'Lopez', 'Gonzalez', 'Wilson', 'Anderson', 'Thomas', 'Taylor', 'Moore', 'Jackson', 'Martin']\n",
    "    \n",
    "    names = [f\"{np.random.choice(first_names)} {np.random.choice(last_names)}\" for _ in range(n_students)]\n",
    "    \n",
    "    # Generate class levels\n",
    "    classes = np.random.choice(['9A', '9B', '10A', '10B', '11A', '11B', '12A', '12B'], n_students)\n",
    "    \n",
    "    # Generate correlated cognitive skills\n",
    "    # Base cognitive abilities with some correlation\n",
    "    base_ability = np.random.normal(70, 15, n_students)\n",
    "    base_ability = np.clip(base_ability, 30, 95)\n",
    "    \n",
    "    # Generate individual cognitive skills with realistic correlations\n",
    "    comprehension = base_ability + np.random.normal(0, 8, n_students)\n",
    "    attention = base_ability + np.random.normal(0, 10, n_students)\n",
    "    focus = 0.7 * attention + 0.3 * base_ability + np.random.normal(0, 6, n_students)\n",
    "    retention = 0.6 * comprehension + 0.4 * base_ability + np.random.normal(0, 7, n_students)\n",
    "    \n",
    "    # Clip values to realistic ranges\n",
    "    comprehension = np.clip(comprehension, 20, 100)\n",
    "    attention = np.clip(attention, 15, 100)\n",
    "    focus = np.clip(focus, 15, 100)\n",
    "    retention = np.clip(retention, 20, 100)\n",
    "    \n",
    "    # Generate assessment scores based on cognitive skills with realistic weights\n",
    "    assessment_score = (0.3 * comprehension + 0.25 * attention + 0.2 * focus + 0.25 * retention + \n",
    "                       np.random.normal(0, 8, n_students))\n",
    "    assessment_score = np.clip(assessment_score, 30, 100)\n",
    "    \n",
    "    # Generate engagement time based on performance and motivation\n",
    "    base_engagement = 15 + (assessment_score - 50) * 0.2  # Higher performers tend to study more\n",
    "    engagement_time = base_engagement + np.random.normal(0, 5, n_students)\n",
    "    engagement_time = np.clip(engagement_time, 5, 40)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'student_id': student_ids,\n",
    "        'name': names,\n",
    "        'class': classes,\n",
    "        'comprehension': np.round(comprehension, 1),\n",
    "        'attention': np.round(attention, 1),\n",
    "        'focus': np.round(focus, 1),\n",
    "        'retention': np.round(retention, 1),\n",
    "        'assessment_score': np.round(assessment_score, 1),\n",
    "        'engagement_time': np.round(engagement_time, 1)\n",
    "    })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate the dataset\n",
    "df = generate_synthetic_student_data(500)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "Let's explore the dataset to understand the distribution of variables and identify patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots for cognitive skills and performance\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Distribution of Cognitive Skills and Performance Metrics', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot distributions\n",
    "cognitive_vars = ['comprehension', 'attention', 'focus', 'retention', 'assessment_score', 'engagement_time']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c']\n",
    "\n",
    "for i, (var, color) in enumerate(zip(cognitive_vars, colors)):\n",
    "    row, col = i // 3, i % 3\n",
    "    axes[row, col].hist(df[var], bins=25, alpha=0.7, color=color, edgecolor='black')\n",
    "    axes[row, col].set_title(f'{var.replace(\"_\", \" \").title()} Distribution', fontweight='bold')\n",
    "    axes[row, col].set_xlabel(var.replace('_', ' ').title())\n",
    "    axes[row, col].set_ylabel('Frequency')\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Graph Justification - Distribution Analysis:\")\n",
    "print(\"These histograms show the distribution of each variable, helping us identify:\")\n",
    "print(\"• Normal vs skewed distributions\")\n",
    "print(\"• Potential outliers\")\n",
    "print(\"• Data quality and realistic value ranges\")\n",
    "print(\"• Whether transformations might be needed for modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Correlation Analysis\n",
    "\n",
    "Analyzing correlations between cognitive skills and academic performance to understand which factors most strongly predict success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "numeric_cols = ['comprehension', 'attention', 'focus', 'retention', 'assessment_score', 'engagement_time']\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            cmap='RdYlBu_r', \n",
    "            center=0,\n",
    "            square=True,\n",
    "            fmt='.3f',\n",
    "            cbar_kws={'label': 'Correlation Coefficient'})\n",
    "plt.title('Correlation Matrix: Cognitive Skills vs Performance', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Graph Justification - Correlation Heatmap:\")\n",
    "print(\"This heatmap reveals relationships between variables, showing:\")\n",
    "print(\"• Which cognitive skills are most correlated with assessment scores\")\n",
    "print(\"• Inter-correlations between cognitive abilities\")\n",
    "print(\"• Potential multicollinearity issues for modeling\")\n",
    "print(\"• Strength and direction of relationships\")\n",
    "\n",
    "# Print strongest correlations with assessment_score\n",
    "assessment_corr = correlation_matrix['assessment_score'].drop('assessment_score').sort_values(ascending=False)\n",
    "print(\"\\n🎯 Strongest Predictors of Assessment Score:\")\n",
    "for skill, corr in assessment_corr.items():\n",
    "    print(f\"• {skill.replace('_', ' ').title()}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots showing relationships with assessment scores\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Cognitive Skills vs Assessment Score Relationships', fontsize=16, fontweight='bold')\n",
    "\n",
    "skills = ['comprehension', 'attention', 'focus', 'retention']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "\n",
    "for i, (skill, color) in enumerate(zip(skills, colors)):\n",
    "    row, col = i // 2, i % 2\n",
    "    axes[row, col].scatter(df[skill], df['assessment_score'], alpha=0.6, color=color, s=50)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(df[skill], df['assessment_score'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[row, col].plot(df[skill], p(df[skill]), \"r--\", alpha=0.8, linewidth=2)\n",
    "    \n",
    "    axes[row, col].set_xlabel(f'{skill.replace(\"_\", \" \").title()} Score')\n",
    "    axes[row, col].set_ylabel('Assessment Score')\n",
    "    axes[row, col].set_title(f'{skill.replace(\"_\", \" \").title()} vs Assessment Score\\n(r = {correlation_matrix.loc[skill, \"assessment_score\"]:.3f})')\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Graph Justification - Scatter Plots:\")\n",
    "print(\"These scatter plots with trend lines help visualize:\")\n",
    "print(\"• Linear vs non-linear relationships\")\n",
    "print(\"• Strength of individual predictors\")\n",
    "print(\"• Outliers and data patterns\")\n",
    "print(\"• Whether simple linear models will be effective\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Machine Learning Model Development\n",
    "\n",
    "Building and comparing multiple ML models to predict assessment scores based on cognitive skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "feature_cols = ['comprehension', 'attention', 'focus', 'retention', 'engagement_time']\n",
    "X = df[feature_cols]\n",
    "y = df['assessment_score']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features for algorithms that need it\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} students\")\n",
    "print(f\"Test set size: {X_test.shape[0]} students\")\n",
    "print(f\"Features used: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train multiple models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=0.1),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'SVR': SVR(kernel='rbf', C=100, gamma=0.1)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Use scaled data for SVR, original for tree-based models\n",
    "    if name == 'SVR':\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R²': r2,\n",
    "        'CV_R²_mean': cv_scores.mean(),\n",
    "        'CV_R²_std': cv_scores.std()\n",
    "    }\n",
    "    \n",
    "    predictions[name] = y_pred\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  RMSE: {rmse:.3f}\")\n",
    "    print(f\"  MAE: {mae:.3f}\")\n",
    "    print(f\"  R²: {r2:.3f}\")\n",
    "    print(f\"  CV R² (mean ± std): {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "    print()\n",
    "\n",
    "# Convert results to DataFrame for easier visualization\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.sort_values('R²', ascending=False)\n",
    "print(\"\\n🏆 Model Performance Ranking:\")\n",
    "print(results_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance comparison visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# R² Score comparison\n",
    "axes[0].bar(results_df.index, results_df['R²'], color='skyblue', alpha=0.8, edgecolor='navy')\n",
    "axes[0].set_title('R² Score Comparison', fontweight='bold')\n",
    "axes[0].set_ylabel('R² Score')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE comparison\n",
    "axes[1].bar(results_df.index, results_df['RMSE'], color='lightcoral', alpha=0.8, edgecolor='darkred')\n",
    "axes[1].set_title('RMSE Comparison (Lower is Better)', fontweight='bold')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Cross-validation scores with error bars\n",
    "axes[2].bar(results_df.index, results_df['CV_R²_mean'], \n",
    "           yerr=results_df['CV_R²_std'], \n",
    "           color='lightgreen', alpha=0.8, edgecolor='darkgreen', capsize=5)\n",
    "axes[2].set_title('Cross-Validation R² (with std dev)', fontweight='bold')\n",
    "axes[2].set_ylabel('CV R² Score')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Graph Justification - Model Performance Comparison:\")\n",
    "print(\"These bar charts help compare models across multiple metrics:\")\n",
    "print(\"• R² shows explained variance (higher is better)\")\n",
    "print(\"• RMSE shows prediction error magnitude (lower is better)\")\n",
    "print(\"• CV scores show model stability and generalization\")\n",
    "print(\"• Error bars indicate model consistency across folds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction vs Actual scatter plots for top 3 models\n",
    "top_models = results_df.head(3).index.tolist()\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Predicted vs Actual Assessment Scores (Top 3 Models)', fontsize=16, fontweight='bold')\n",
    "\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "for i, (model_name, color) in enumerate(zip(top_models, colors)):\n",
    "    y_pred = predictions[model_name]\n",
    "    \n",
    "    axes[i].scatter(y_test, y_pred, alpha=0.6, color=color, s=50)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_test.min(), y_pred.min())\n",
    "    max_val = max(y_test.max(), y_pred.max())\n",
    "    axes[i].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, alpha=0.8)\n",
    "    \n",
    "    axes[i].set_xlabel('Actual Assessment Score')\n",
    "    axes[i].set_ylabel('Predicted Assessment Score')\n",
    "    axes[i].set_title(f'{model_name}\\nR² = {results_df.loc[model_name, \"R²\"]:.3f}')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add correlation coefficient\n",
    "    corr = np.corrcoef(y_test, y_pred)[0, 1]\n",
    "    axes[i].text(0.05, 0.95, f'Correlation: {corr:.3f}', \n",
    "                transform=axes[i].transAxes, fontsize=10, \n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Graph Justification - Predicted vs Actual Plots:\")\n",
    "print(\"These scatter plots show model prediction quality:\")\n",
    "print(\"• Points closer to the diagonal line indicate better predictions\")\n",
    "print(\"• Spread around the line shows prediction variance\")\n",
    "print(\"• Systematic deviations reveal model bias\")\n",
    "print(\"• Helps identify which model makes most accurate predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis\n",
    "\n",
    "Understanding which cognitive skills are most important for predicting academic performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest (best performing model)\n",
    "best_model = models['Random Forest']\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': best_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.bar(feature_importance['feature'], feature_importance['importance'], \n",
    "               color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6'], \n",
    "               alpha=0.8, edgecolor='black')\n",
    "\n",
    "plt.title('Feature Importance for Assessment Score Prediction\\n(Random Forest Model)', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Cognitive Skills & Engagement', fontsize=12)\n",
    "plt.ylabel('Importance Score', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, importance in zip(bars, feature_importance['importance']):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "             f'{importance:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🎯 Feature Importance Ranking:\")\n",
    "for i, row in feature_importance.iterrows():\n",
    "    print(f\"{row['feature'].replace('_', ' ').title()}: {row['importance']:.3f}\")\n",
    "\n",
    "print(\"\\n📊 Graph Justification - Feature Importance:\")\n",
    "print(\"This bar chart reveals which cognitive skills matter most:\")\n",
    "print(\"• Identifies key predictors for targeted interventions\")\n",
    "print(\"• Shows relative contribution of each skill\")\n",
    "print(\"• Helps prioritize educational focus areas\")\n",
    "print(\"• Guides feature selection for model optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Student Clustering Analysis\n",
    "\n",
    "Clustering students into learning personas based on their cognitive skill profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for clustering (cognitive skills only)\n",
    "clustering_features = ['comprehension', 'attention', 'focus', 'retention']\n",
    "X_cluster = df[clustering_features]\n",
    "X_cluster_scaled = StandardScaler().fit_transform(X_cluster)\n",
    "\n",
    "# Determine optimal number of clusters using elbow method\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_cluster_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_cluster_scaled, kmeans.labels_))\n",
    "\n",
    "# Plot elbow curve and silhouette scores\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Optimal Number of Clusters Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Elbow method\n",
    "axes[0].plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_title('Elbow Method for Optimal k', fontweight='bold')\n",
    "axes[0].set_xlabel('Number of Clusters (k)')\n",
    "axes[0].set_ylabel('Inertia (Within-cluster sum of squares)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette scores\n",
    "axes[1].plot(k_range, silhouette_scores, 'ro-', linewidth=2, markersize=8)\n",
    "axes[1].set_title('Silhouette Score for Different k', fontweight='bold')\n",
    "axes[1].set_xlabel('Number of Clusters (k)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Choose optimal k (highest silhouette score)\n",
    "optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"\\n🎯 Optimal number of clusters: {optimal_k}\")\n",
    "print(f\"Silhouette score: {max(silhouette_scores):.3f}\")\n",
    "\n",
    "print(\"\\n📊 Graph Justification - Cluster Optimization:\")\n",
    "print(\"These plots help determine the optimal number of student personas:\")\n",
    "print(\"• Elbow method shows diminishing returns in cluster compactness\")\n",
    "print(\"• Silhouette score measures cluster separation quality\")\n",
    "print(\"• Higher silhouette scores indicate better-defined clusters\")\n",
    "print(\"• Helps balance interpretability with statistical validity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform final clustering with optimal k\n",
    "final_kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "cluster_labels = final_kmeans.fit_predict(X_cluster_scaled)\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df['learning_persona'] = cluster_labels\n",
    "\n",
    "# Analyze cluster characteristics\n",
    "cluster_summary = df.groupby('learning_persona')[clustering_features + ['assessment_score', 'engagement_time']].mean()\n",
    "cluster_counts = df['learning_persona'].value_counts().sort_index()\n",
    "\n",
    "print(\"\\n🎭 Learning Persona Characteristics:\")\n",
    "print(cluster_summary.round(2))\n",
    "print(\"\\n👥 Students per Persona:\")\n",
    "print(cluster_counts)\n",
    "\n",
    "# Define persona names based on characteristics\n",
    "persona_names = {\n",
    "    0: \"Balanced Achievers\",\n",
    "    1: \"High Performers\", \n",
    "    2: \"Developing Learners\",\n",
    "    3: \"Focus Challengers\"\n",
    "}\n",
    "\n",
    "# Update the names based on actual cluster characteristics\n",
    "sorted_clusters = cluster_summary.sort_values('assessment_score', ascending=False)\n",
    "persona_mapping = {}\n",
    "names = [\"High Performers\", \"Strong Achievers\", \"Developing Learners\", \"Support Needed\"]\n",
    "\n",
    "for i, (cluster_id, _) in enumerate(sorted_clusters.iterrows()):\n",
    "    if i < len(names):\n",
    "        persona_mapping[cluster_id] = names[i]\n",
    "    else:\n",
    "        persona_mapping[cluster_id] = f\"Persona {cluster_id}\"\n",
    "\n",
    "df['persona_name'] = df['learning_persona'].map(persona_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters using PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_cluster_scaled)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6']\n",
    "\n",
    "for i in range(optimal_k):\n",
    "    mask = cluster_labels == i\n",
    "    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "               c=colors[i], label=f'{persona_mapping[i]} (n={cluster_counts[i]})', \n",
    "               alpha=0.7, s=60)\n",
    "\n",
    "# Plot cluster centers\n",
    "centers_pca = pca.transform(final_kmeans.cluster_centers_)\n",
    "plt.scatter(centers_pca[:, 0], centers_pca[:, 1], \n",
    "           c='black', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "\n",
    "plt.title('Student Learning Personas\\n(PCA Visualization of Cognitive Skills)', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel(f'First Principal Component ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "plt.ylabel(f'Second Principal Component ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n📊 PCA Explained Variance: {pca.explained_variance_ratio_.sum():.1%}\")\n",
    "print(\"\\n📊 Graph Justification - PCA Cluster Visualization:\")\n",
    "print(\"This scatter plot shows student personas in 2D space:\")\n",
    "print(\"• Each color represents a distinct learning persona\")\n",
    "print(\"• Cluster separation indicates how distinct the personas are\")\n",
    "print(\"• Centroids show the 'typical' student in each persona\")\n",
    "print(\"• PCA reduces 4D cognitive skills to 2D for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar chart for cluster profiles\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 16), subplot_kw=dict(projection='polar'))\n",
    "fig.suptitle('Learning Persona Cognitive Profiles (Radar Charts)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Prepare data for radar chart\n",
    "skills = clustering_features\n",
    "angles = np.linspace(0, 2 * np.pi, len(skills), endpoint=False).tolist()\n",
    "angles += angles[:1]  # Complete the circle\n",
    "\n",
    "for i in range(optimal_k):\n",
    "    row, col = i // 2, i % 2\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Get cluster data\n",
    "    cluster_data = cluster_summary.loc[i, skills].tolist()\n",
    "    cluster_data += cluster_data[:1]  # Complete the circle\n",
    "    \n",
    "    # Plot\n",
    "    ax.plot(angles, cluster_data, 'o-', linewidth=2, color=colors[i], alpha=0.8)\n",
    "    ax.fill(angles, cluster_data, alpha=0.25, color=colors[i])\n",
    "    \n",
    "    # Customize\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels([skill.replace('_', ' ').title() for skill in skills])\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_title(f'{persona_mapping[i]}\\n(n={cluster_counts[i]} students)', \n",
    "                fontweight='bold', pad=20)\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Graph Justification - Radar Charts:\")\n",
    "print(\"These radar charts show the cognitive profile of each persona:\")\n",
    "print(\"• Each axis represents a different cognitive skill\")\n",
    "print(\"• Larger areas indicate stronger overall cognitive abilities\")\n",
    "print(\"• Shape differences reveal unique strength/weakness patterns\")\n",
    "print(\"• Helps educators understand each persona's needs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Persona-Based Insights and Recommendations\n",
    "\n",
    "Generating actionable insights for each learning persona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed persona insights\n",
    "def generate_persona_insights(cluster_id, cluster_data, persona_name, count):\n",
    "    insights = {\n",
    "        'name': persona_name,\n",
    "        'count': count,\n",
    "        'percentage': (count / len(df)) * 100,\n",
    "        'avg_assessment': cluster_data['assessment_score'],\n",
    "        'avg_engagement': cluster_data['engagement_time'],\n",
    "        'strengths': [],\n",
    "        'challenges': [],\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Identify strengths and challenges\n",
    "    overall_means = df[clustering_features].mean()\n",
    "    \n",
    "    for skill in clustering_features:\n",
    "        if cluster_data[skill] > overall_means[skill] + 5:\n",
    "            insights['strengths'].append(skill.replace('_', ' ').title())\n",
    "        elif cluster_data[skill] < overall_means[skill] - 5:\n",
    "            insights['challenges'].append(skill.replace('_', ' ').title())\n",
    "    \n",
    "    # Generate recommendations based on profile\n",
    "    if cluster_data['assessment_score'] > 80:\n",
    "        insights['recommendations'].extend([\n",
    "            \"Provide advanced challenges and enrichment activities\",\n",
    "            \"Consider peer tutoring or leadership roles\",\n",
    "            \"Encourage independent research projects\"\n",
    "        ])\n",
    "    elif cluster_data['assessment_score'] > 65:\n",
    "        insights['recommendations'].extend([\n",
    "            \"Maintain current learning strategies\",\n",
    "            \"Focus on consistency and time management\",\n",
    "            \"Provide moderate challenges to promote growth\"\n",
    "        ])\n",
    "    else:\n",
    "        insights['recommendations'].extend([\n",
    "            \"Provide additional support and scaffolding\",\n",
    "            \"Focus on building foundational skills\",\n",
    "            \"Consider one-on-one tutoring or small group instruction\"\n",
    "        ])\n",
    "    \n",
    "    # Add skill-specific recommendations\n",
    "    if 'Attention' in insights['challenges']:\n",
    "        insights['recommendations'].append(\"Implement attention-building exercises and mindfulness practices\")\n",
    "    if 'Focus' in insights['challenges']:\n",
    "        insights['recommendations'].append(\"Use shorter learning sessions with frequent breaks\")\n",
    "    if 'Comprehension' in insights['challenges']:\n",
    "        insights['recommendations'].append(\"Provide visual aids and multiple explanation methods\")\n",
    "    if 'Retention' in insights['challenges']:\n",
    "        insights['recommendations'].append(\"Implement spaced repetition and memory techniques\")\n",
    "    \n",
    "    return insights\n",
    "\n",
    "# Generate insights for each persona\n",
    "persona_insights = {}\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_data = cluster_summary.loc[cluster_id]\n",
    "    persona_name = persona_mapping[cluster_id]\n",
    "    count = cluster_counts[cluster_id]\n",
    "    \n",
    "    persona_insights[cluster_id] = generate_persona_insights(\n",
    "        cluster_id, cluster_data, persona_name, count\n",
    "    )\n",
    "\n",
    "# Display insights\n",
    "print(\"\\n🎭 LEARNING PERSONA INSIGHTS & RECOMMENDATIONS\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for cluster_id, insights in persona_insights.items():\n",
    "    print(f\"\\n📊 {insights['name']}\")\n",
    "    print(f\"   Students: {insights['count']} ({insights['percentage']:.1f}% of total)\")\n",
    "    print(f\"   Avg Assessment Score: {insights['avg_assessment']:.1f}\")\n",
    "    print(f\"   Avg Engagement Time: {insights['avg_engagement']:.1f} hours/week\")\n",
    "    \n",
    "    if insights['strengths']:\n",
    "        print(f\"   💪 Strengths: {', '.join(insights['strengths'])}\")\n",
    "    \n",
    "    if insights['challenges']:\n",
    "        print(f\"   ⚠️  Challenges: {', '.join(insights['challenges'])}\")\n",
    "    \n",
    "    print(f\"   🎯 Recommendations:\")\n",
    "    for rec in insights['recommendations']:\n",
    "        print(f\"      • {rec}\")\n",
    "    \n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Deployment Preparation\n",
    "\n",
    "Preparing the best model for integration with the dashboard application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and preprocessing components\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Get the best performing model\n",
    "best_model_name = results_df.index[0]\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
    "print(f\"R² Score: {results_df.loc[best_model_name, 'R²']:.3f}\")\n",
    "print(f\"RMSE: {results_df.loc[best_model_name, 'RMSE']:.3f}\")\n",
    "\n",
    "# Create model artifacts for deployment\n",
    "model_artifacts = {\n",
    "    'model': best_model,\n",
    "    'scaler': scaler if best_model_name == 'SVR' else None,\n",
    "    'feature_columns': feature_cols,\n",
    "    'clustering_model': final_kmeans,\n",
    "    'clustering_scaler': StandardScaler().fit(X_cluster),\n",
    "    'persona_mapping': persona_mapping,\n",
    "    'model_performance': results_df.loc[best_model_name].to_dict()\n",
    "}\n",
    "\n",
    "# Save model artifacts (in a real scenario)\n",
    "print(\"\\n💾 Model artifacts prepared for deployment:\")\n",
    "print(f\"   • Trained {best_model_name} model\")\n",
    "print(f\"   • Feature preprocessing pipeline\")\n",
    "print(f\"   • Student clustering model\")\n",
    "print(f\"   • Persona mapping and insights\")\n",
    "print(f\"   • Performance metrics and validation results\")\n",
    "\n",
    "# Create prediction function for dashboard integration\n",
    "def predict_student_performance(comprehension, attention, focus, retention, engagement_time):\n",
    "    \"\"\"\n",
    "    Predict student assessment score and learning persona.\n",
    "    \n",
    "    Args:\n",
    "        comprehension (float): Comprehension score (0-100)\n",
    "        attention (float): Attention score (0-100)\n",
    "        focus (float): Focus score (0-100)\n",
    "        retention (float): Retention score (0-100)\n",
    "        engagement_time (float): Weekly engagement time in hours\n",
    "    \n",
    "    Returns:\n",
    "        dict: Prediction results including score, persona, and confidence\n",
    "    \"\"\"\n",
    "    # Prepare input data\n",
    "    input_data = np.array([[comprehension, attention, focus, retention, engagement_time]])\n",
    "    \n",
    "    # Predict assessment score\n",
    "    if best_model_name == 'SVR':\n",
    "        input_scaled = scaler.transform(input_data)\n",
    "        predicted_score = best_model.predict(input_scaled)[0]\n",
    "    else:\n",
    "        predicted_score = best_model.predict(input_data)[0]\n",
    "    \n",
    "    # Predict learning persona\n",
    "    cognitive_data = np.array([[comprehension, attention, focus, retention]])\n",
    "    cognitive_scaled = StandardScaler().fit(X_cluster).transform(cognitive_data)\n",
    "    persona_id = final_kmeans.predict(cognitive_scaled)[0]\n",
    "    persona_name = persona_mapping[persona_id]\n",
    "    \n",
    "    # Calculate confidence (distance to cluster center)\n",
    "    distances = final_kmeans.transform(cognitive_scaled)[0]\n",
    "    confidence = 1 / (1 + distances[persona_id])  # Higher confidence for closer points\n",
    "    \n",
    "    return {\n",
    "        'predicted_score': round(predicted_score, 1),\n",
    "        'persona_id': int(persona_id),\n",
    "        'persona_name': persona_name,\n",
    "        'confidence': round(confidence, 3),\n",
    "        'model_used': best_model_name,\n",
    "        'model_r2': round(results_df.loc[best_model_name, 'R²'], 3)\n",
    "    }\n",
    "\n",
    "# Test the prediction function\n",
    "test_prediction = predict_student_performance(75, 80, 70, 85, 20)\n",
    "print(\"\\n🧪 Test Prediction:\")\n",
    "print(f\"   Input: Comprehension=75, Attention=80, Focus=70, Retention=85, Engagement=20h\")\n",
    "print(f\"   Predicted Score: {test_prediction['predicted_score']}\")\n",
    "print(f\"   Learning Persona: {test_prediction['persona_name']}\")\n",
    "print(f\"   Confidence: {test_prediction['confidence']}\")\n",
    "print(f\"   Model: {test_prediction['model_used']} (R² = {test_prediction['model_r2']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Key Findings\n",
    "\n",
    "Comprehensive summary of the analysis results and actionable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"📊 STUDENT COGNITIVE SKILLS & PERFORMANCE ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n🎯 KEY FINDINGS:\")\n",
    "print(f\"   • Dataset: {len(df)} students across {df['class'].nunique()} classes\")\n",
    "print(f\"   • Best ML Model: {best_model_name} (R² = {results_df.loc[best_model_name, 'R²']:.3f})\")\n",
    "print(f\"   • Model Accuracy: Can predict assessment scores with {results_df.loc[best_model_name, 'RMSE']:.1f} point RMSE\")\n",
    "print(f\"   • Learning Personas: {optimal_k} distinct student personas identified\")\n",
    "\n",
    "print(\"\\n🔍 CORRELATION INSIGHTS:\")\n",
    "assessment_corr = df[clustering_features].corrwith(df['assessment_score']).sort_values(ascending=False)\n",
    "for skill, corr in assessment_corr.items():\n",
    "    strength = \"Strong\" if abs(corr) > 0.7 else \"Moderate\" if abs(corr) > 0.5 else \"Weak\"\n",
    "    print(f\"   • {skill.replace('_', ' ').title()}: {corr:.3f} ({strength} correlation)\")\n",
    "\n",
    "print(\"\\n🎭 LEARNING PERSONAS:\")\n",
    "for cluster_id, insights in persona_insights.items():\n",
    "    print(f\"   • {insights['name']}: {insights['count']} students ({insights['percentage']:.1f}%)\")\n",
    "    print(f\"     Average Score: {insights['avg_assessment']:.1f}, Engagement: {insights['avg_engagement']:.1f}h/week\")\n",
    "\n",
    "print(\"\\n🚀 ACTIONABLE RECOMMENDATIONS:\")\n",
    "print(\"   • Focus on comprehension and retention skills for maximum impact\")\n",
    "print(\"   • Implement persona-specific teaching strategies\")\n",
    "print(\"   • Use ML predictions to identify at-risk students early\")\n",
    "print(\"   • Monitor engagement time as a leading indicator\")\n",
    "print(\"   • Provide targeted interventions based on cognitive profiles\")\n",
    "\n",
    "print(\"\\n📈 MODEL DEPLOYMENT READY:\")\n",
    "print(\"   • Trained model can predict assessment scores from cognitive skills\")\n",
    "print(\"   • Clustering model identifies student learning personas\")\n",
    "print(\"   • Integration-ready functions for dashboard application\")\n",
    "print(\"   • Comprehensive validation and performance metrics included\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ ANALYSIS COMPLETE - Ready for Dashboard Integration\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
