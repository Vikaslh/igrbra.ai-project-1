{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Machine Learning Analysis for Student Performance Prediction\n",
    "\n",
    "This notebook demonstrates a complete machine learning workflow including:\n",
    "- Data preprocessing and feature engineering\n",
    "- Multiple ML algorithms comparison\n",
    "- Comprehensive model evaluation\n",
    "- Advanced visualizations with justifications\n",
    "- Performance optimization techniques\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Loading and Exploration](#data-loading)\n",
    "2. [Data Preprocessing](#preprocessing)\n",
    "3. [Feature Engineering](#feature-engineering)\n",
    "4. [Model Training and Comparison](#model-training)\n",
    "5. [Model Evaluation](#evaluation)\n",
    "6. [Advanced Analysis](#advanced-analysis)\n",
    "7. [Conclusions and Recommendations](#conclusions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration <a id=\"data-loading\"></a>\n",
    "\n",
    "We'll generate a comprehensive synthetic dataset that simulates real-world student performance data with various cognitive skills and environmental factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comprehensive_dataset(n_samples=5000):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive synthetic dataset for student performance analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    n_samples (int): Number of samples to generate\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Generated dataset\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate base features\n",
    "    data = {\n",
    "        'student_id': range(1, n_samples + 1),\n",
    "        'age': np.random.normal(16, 2, n_samples).clip(13, 20),\n",
    "        'grade_level': np.random.choice([9, 10, 11, 12], n_samples, p=[0.25, 0.25, 0.25, 0.25]),\n",
    "        'gender': np.random.choice(['Male', 'Female', 'Other'], n_samples, p=[0.45, 0.45, 0.1]),\n",
    "        'socioeconomic_status': np.random.choice(['Low', 'Medium', 'High'], n_samples, p=[0.3, 0.5, 0.2]),\n",
    "        \n",
    "        # Cognitive skills (0-100 scale)\n",
    "        'comprehension': np.random.beta(2, 2, n_samples) * 100,\n",
    "        'attention_span': np.random.beta(2, 2, n_samples) * 100,\n",
    "        'memory_retention': np.random.beta(2, 2, n_samples) * 100,\n",
    "        'problem_solving': np.random.beta(2, 2, n_samples) * 100,\n",
    "        'critical_thinking': np.random.beta(2, 2, n_samples) * 100,\n",
    "        \n",
    "        # Study habits and environment\n",
    "        'study_hours_per_week': np.random.gamma(2, 5, n_samples).clip(0, 50),\n",
    "        'sleep_hours_per_night': np.random.normal(7.5, 1.5, n_samples).clip(4, 12),\n",
    "        'extracurricular_activities': np.random.poisson(2, n_samples).clip(0, 8),\n",
    "        'family_support': np.random.beta(3, 2, n_samples) * 10,\n",
    "        'teacher_rating': np.random.beta(3, 2, n_samples) * 10,\n",
    "        \n",
    "        # Technology and resources\n",
    "        'computer_access': np.random.choice([0, 1], n_samples, p=[0.2, 0.8]),\n",
    "        'internet_quality': np.random.choice(['Poor', 'Fair', 'Good', 'Excellent'], n_samples, p=[0.1, 0.2, 0.4, 0.3]),\n",
    "        'learning_resources': np.random.beta(2, 2, n_samples) * 10,\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create realistic correlations for assessment scores\n",
    "    # Assessment score is influenced by multiple factors with realistic noise\n",
    "    assessment_base = (\n",
    "        0.3 * df['comprehension'] +\n",
    "        0.25 * df['problem_solving'] +\n",
    "        0.2 * df['critical_thinking'] +\n",
    "        0.15 * df['attention_span'] +\n",
    "        0.1 * df['memory_retention'] +\n",
    "        0.05 * df['study_hours_per_week'] +\n",
    "        0.03 * df['family_support'] * 10 +\n",
    "        0.02 * df['teacher_rating'] * 10\n",
    "    )\n",
    "    \n",
    "    # Add socioeconomic and demographic effects\n",
    "    ses_effect = df['socioeconomic_status'].map({'Low': -5, 'Medium': 0, 'High': 5})\n",
    "    grade_effect = (df['grade_level'] - 9) * 2  # Higher grades slightly better\n",
    "    \n",
    "    # Add realistic noise and constraints\n",
    "    noise = np.random.normal(0, 8, n_samples)\n",
    "    df['assessment_score'] = (assessment_base + ses_effect + grade_effect + noise).clip(0, 100)\n",
    "    \n",
    "    # Create engagement metrics\n",
    "    df['class_participation'] = (df['attention_span'] * 0.6 + df['family_support'] * 4 + np.random.normal(0, 10, n_samples)).clip(0, 100)\n",
    "    df['homework_completion'] = (df['study_hours_per_week'] * 1.5 + df['family_support'] * 3 + np.random.normal(0, 8, n_samples)).clip(0, 100)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate the dataset\n",
    "df = generate_comprehensive_dataset(5000)\n",
    "print(f\"Dataset generated with {len(df)} samples and {len(df.columns)} features\")\n",
    "print(\"\\nDataset shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset overview and basic statistics\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Total features: {len(df.columns)}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n=== DATA TYPES ===\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n=== MISSING VALUES ===\")\n",
    "missing_values = df.isnull().sum()\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"No missing values found!\")\n",
    "else:\n",
    "    print(missing_values[missing_values > 0])\n",
    "\n",
    "print(\"\\n=== BASIC STATISTICS ===\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization - Exploratory Data Analysis\n",
    "\n",
    "**Graph Justification**: The following visualizations help us understand:\n",
    "1. **Distribution plots**: Identify data skewness and outliers\n",
    "2. **Correlation heatmap**: Understand feature relationships\n",
    "3. **Box plots**: Compare distributions across categories\n",
    "4. **Scatter plots**: Visualize relationships between key variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive EDA visualizations\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "fig.suptitle('Exploratory Data Analysis - Key Features Distribution', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Assessment Score Distribution\n",
    "axes[0, 0].hist(df['assessment_score'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Assessment Score Distribution')\n",
    "axes[0, 0].set_xlabel('Assessment Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].axvline(df['assessment_score'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"assessment_score\"].mean():.1f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Cognitive Skills Distribution\n",
    "cognitive_skills = ['comprehension', 'attention_span', 'memory_retention', 'problem_solving', 'critical_thinking']\n",
    "df[cognitive_skills].boxplot(ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Cognitive Skills Distribution')\n",
    "axes[0, 1].set_ylabel('Score (0-100)')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Study Hours vs Assessment Score\n",
    "axes[0, 2].scatter(df['study_hours_per_week'], df['assessment_score'], alpha=0.6, color='green')\n",
    "axes[0, 2].set_title('Study Hours vs Assessment Score')\n",
    "axes[0, 2].set_xlabel('Study Hours per Week')\n",
    "axes[0, 2].set_ylabel('Assessment Score')\n",
    "\n",
    "# 4. Socioeconomic Status Impact\n",
    "sns.boxplot(data=df, x='socioeconomic_status', y='assessment_score', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Assessment Score by Socioeconomic Status')\n",
    "axes[1, 0].set_ylabel('Assessment Score')\n",
    "\n",
    "# 5. Grade Level Performance\n",
    "grade_performance = df.groupby('grade_level')['assessment_score'].mean()\n",
    "axes[1, 1].bar(grade_performance.index, grade_performance.values, color='orange', alpha=0.7)\n",
    "axes[1, 1].set_title('Average Assessment Score by Grade Level')\n",
    "axes[1, 1].set_xlabel('Grade Level')\n",
    "axes[1, 1].set_ylabel('Average Assessment Score')\n",
    "\n",
    "# 6. Sleep vs Performance\n",
    "axes[1, 2].scatter(df['sleep_hours_per_night'], df['assessment_score'], alpha=0.6, color='purple')\n",
    "axes[1, 2].set_title('Sleep Hours vs Assessment Score')\n",
    "axes[1, 2].set_xlabel('Sleep Hours per Night')\n",
    "axes[1, 2].set_ylabel('Assessment Score')\n",
    "\n",
    "# 7. Family Support Impact\n",
    "axes[2, 0].scatter(df['family_support'], df['assessment_score'], alpha=0.6, color='red')\n",
    "axes[2, 0].set_title('Family Support vs Assessment Score')\n",
    "axes[2, 0].set_xlabel('Family Support (0-10)')\n",
    "axes[2, 0].set_ylabel('Assessment Score')\n",
    "\n",
    "# 8. Technology Access Impact\n",
    "tech_performance = df.groupby('computer_access')['assessment_score'].mean()\n",
    "axes[2, 1].bar(['No Computer', 'Has Computer'], tech_performance.values, color=['red', 'green'], alpha=0.7)\n",
    "axes[2, 1].set_title('Assessment Score by Computer Access')\n",
    "axes[2, 1].set_ylabel('Average Assessment Score')\n",
    "\n",
    "# 9. Age Distribution\n",
    "axes[2, 2].hist(df['age'], bins=20, alpha=0.7, color='brown', edgecolor='black')\n",
    "axes[2, 2].set_title('Age Distribution')\n",
    "axes[2, 2].set_xlabel('Age')\n",
    "axes[2, 2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== KEY INSIGHTS FROM EDA ===\")\n",
    "print(f\"1. Average assessment score: {df['assessment_score'].mean():.2f} ¬± {df['assessment_score'].std():.2f}\")\n",
    "print(f\"2. Study hours correlation with performance: {df['study_hours_per_week'].corr(df['assessment_score']):.3f}\")\n",
    "print(f\"3. Sleep hours correlation with performance: {df['sleep_hours_per_night'].corr(df['assessment_score']):.3f}\")\n",
    "print(f\"4. Family support correlation with performance: {df['family_support'].corr(df['assessment_score']):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis\n",
    "# **Graph Justification**: Correlation heatmap reveals multicollinearity and helps with feature selection\n",
    "\n",
    "# Select numerical columns for correlation analysis\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_cols.remove('student_id')  # Remove ID column\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(16, 12))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdYlBu_r', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8}, fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix\\n(Upper triangle masked for clarity)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify highly correlated features with target\n",
    "target_correlations = correlation_matrix['assessment_score'].abs().sort_values(ascending=False)\n",
    "print(\"\\n=== FEATURES MOST CORRELATED WITH ASSESSMENT SCORE ===\")\n",
    "for feature, corr in target_correlations.head(10).items():\n",
    "    if feature != 'assessment_score':\n",
    "        print(f\"{feature}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing <a id=\"preprocessing\"></a>\n",
    "\n",
    "Comprehensive preprocessing pipeline including:\n",
    "- Handling categorical variables\n",
    "- Feature scaling\n",
    "- Outlier detection and treatment\n",
    "- Data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_preprocessing(df):\n",
    "    \"\"\"\n",
    "    Comprehensive preprocessing pipeline for the student dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataframe\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (processed_df, encoders, scalers)\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    encoders = {}\n",
    "    scalers = {}\n",
    "    \n",
    "    print(\"=== PREPROCESSING PIPELINE ===\")\n",
    "    \n",
    "    # 1. Handle categorical variables\n",
    "    print(\"1. Encoding categorical variables...\")\n",
    "    \n",
    "    # Label encoding for ordinal variables\n",
    "    ordinal_mappings = {\n",
    "        'socioeconomic_status': {'Low': 0, 'Medium': 1, 'High': 2},\n",
    "        'internet_quality': {'Poor': 0, 'Fair': 1, 'Good': 2, 'Excellent': 3}\n",
    "    }\n",
    "    \n",
    "    for col, mapping in ordinal_mappings.items():\n",
    "        df_processed[col] = df_processed[col].map(mapping)\n",
    "        encoders[col] = mapping\n",
    "    \n",
    "    # One-hot encoding for nominal variables\n",
    "    nominal_cols = ['gender']\n",
    "    for col in nominal_cols:\n",
    "        dummies = pd.get_dummies(df_processed[col], prefix=col, drop_first=True)\n",
    "        df_processed = pd.concat([df_processed, dummies], axis=1)\n",
    "        df_processed.drop(col, axis=1, inplace=True)\n",
    "        encoders[col] = list(dummies.columns)\n",
    "    \n",
    "    print(f\"   - Encoded {len(ordinal_mappings) + len(nominal_cols)} categorical variables\")\n",
    "    \n",
    "    # 2. Outlier detection and treatment\n",
    "    print(\"2. Detecting and treating outliers...\")\n",
    "    \n",
    "    numerical_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numerical_cols.remove('student_id')\n",
    "    \n",
    "    outlier_counts = {}\n",
    "    for col in numerical_cols:\n",
    "        Q1 = df_processed[col].quantile(0.25)\n",
    "        Q3 = df_processed[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = (df_processed[col] < lower_bound) | (df_processed[col] > upper_bound)\n",
    "        outlier_counts[col] = outliers.sum()\n",
    "        \n",
    "        # Cap outliers instead of removing them\n",
    "        df_processed[col] = df_processed[col].clip(lower_bound, upper_bound)\n",
    "    \n",
    "    total_outliers = sum(outlier_counts.values())\n",
    "    print(f\"   - Detected and capped {total_outliers} outliers across all features\")\n",
    "    \n",
    "    # 3. Feature scaling\n",
    "    print(\"3. Scaling numerical features...\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    feature_cols = [col for col in df_processed.columns if col not in ['student_id', 'assessment_score']]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    df_processed[feature_cols] = scaler.fit_transform(df_processed[feature_cols])\n",
    "    scalers['features'] = scaler\n",
    "    \n",
    "    print(f\"   - Scaled {len(feature_cols)} features using StandardScaler\")\n",
    "    \n",
    "    # 4. Data validation\n",
    "    print(\"4. Validating processed data...\")\n",
    "    \n",
    "    # Check for any remaining issues\n",
    "    missing_after = df_processed.isnull().sum().sum()\n",
    "    infinite_values = np.isinf(df_processed.select_dtypes(include=[np.number])).sum().sum()\n",
    "    \n",
    "    print(f\"   - Missing values after preprocessing: {missing_after}\")\n",
    "    print(f\"   - Infinite values after preprocessing: {infinite_values}\")\n",
    "    print(f\"   - Final dataset shape: {df_processed.shape}\")\n",
    "    \n",
    "    return df_processed, encoders, scalers\n",
    "\n",
    "# Apply preprocessing\n",
    "df_processed, encoders, scalers = comprehensive_preprocessing(df)\n",
    "\n",
    "print(\"\\n=== PREPROCESSING COMPLETE ===\")\n",
    "print(f\"Original features: {len(df.columns)}\")\n",
    "print(f\"Processed features: {len(df_processed.columns)}\")\n",
    "print(f\"Feature columns: {len([col for col in df_processed.columns if col not in ['student_id', 'assessment_score']])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering <a id=\"feature-engineering\"></a>\n",
    "\n",
    "Creating new features to improve model performance:\n",
    "- Interaction features\n",
    "- Polynomial features\n",
    "- Domain-specific features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_engineered_features(df_processed):\n",
    "    \"\"\"\n",
    "    Create engineered features based on domain knowledge.\n",
    "    \n",
    "    Parameters:\n",
    "    df_processed (pd.DataFrame): Preprocessed dataframe\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Dataframe with engineered features\n",
    "    \"\"\"\n",
    "    df_engineered = df_processed.copy()\n",
    "    \n",
    "    print(\"=== FEATURE ENGINEERING ===\")\n",
    "    \n",
    "    # 1. Cognitive Skills Composite Score\n",
    "    cognitive_cols = ['comprehension', 'attention_span', 'memory_retention', 'problem_solving', 'critical_thinking']\n",
    "    df_engineered['cognitive_composite'] = df_engineered[cognitive_cols].mean(axis=1)\n",
    "    print(\"1. Created cognitive composite score\")\n",
    "    \n",
    "    # 2. Study Efficiency (performance per study hour)\n",
    "    df_engineered['study_efficiency'] = df_engineered['assessment_score'] / (df_engineered['study_hours_per_week'] + 1)\n",
    "    print(\"2. Created study efficiency metric\")\n",
    "    \n",
    "    # 3. Work-Life Balance Score\n",
    "    df_engineered['work_life_balance'] = (df_engineered['sleep_hours_per_night'] * 0.4 + \n",
    "                                         (10 - df_engineered['extracurricular_activities']) * 0.3 + \n",
    "                                         (50 - df_engineered['study_hours_per_week']) * 0.3)\n",
    "    print(\"3. Created work-life balance score\")\n",
    "    \n",
    "    # 4. Support System Strength\n",
    "    df_engineered['support_system'] = (df_engineered['family_support'] * 0.6 + \n",
    "                                      df_engineered['teacher_rating'] * 0.4)\n",
    "    print(\"4. Created support system strength metric\")\n",
    "    \n",
    "    # 5. Technology Advantage Score\n",
    "    df_engineered['tech_advantage'] = (df_engineered['computer_access'] * 0.5 + \n",
    "                                      df_engineered['internet_quality'] * 0.3 + \n",
    "                                      df_engineered['learning_resources'] * 0.2)\n",
    "    print(\"5. Created technology advantage score\")\n",
    "    \n",
    "    # 6. Interaction Features\n",
    "    df_engineered['comprehension_x_attention'] = df_engineered['comprehension'] * df_engineered['attention_span']\n",
    "    df_engineered['study_hours_x_family_support'] = df_engineered['study_hours_per_week'] * df_engineered['family_support']\n",
    "    print(\"6. Created interaction features\")\n",
    "    \n",
    "    # 7. Age-Grade Alignment\n",
    "    expected_age = df_engineered['grade_level'] + 5  # Assuming grade 9 = age 14\n",
    "    df_engineered['age_grade_alignment'] = 1 - abs(df_engineered['age'] - expected_age) / 4\n",
    "    print(\"7. Created age-grade alignment feature\")\n",
    "    \n",
    "    print(f\"\\nFeature engineering complete. Added {len(df_engineered.columns) - len(df_processed.columns)} new features.\")\n",
    "    print(f\"Total features now: {len(df_engineered.columns)}\")\n",
    "    \n",
    "    return df_engineered\n",
    "\n",
    "# Apply feature engineering\n",
    "df_engineered = create_engineered_features(df_processed)\n",
    "\n",
    "# Display new features\n",
    "new_features = [col for col in df_engineered.columns if col not in df_processed.columns]\n",
    "print(f\"\\nNew engineered features: {new_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection using statistical methods\n",
    "# **Graph Justification**: Feature importance plots help identify the most predictive features\n",
    "\n",
    "# Prepare data for feature selection\n",
    "feature_cols = [col for col in df_engineered.columns if col not in ['student_id', 'assessment_score']]\n",
    "X = df_engineered[feature_cols]\n",
    "y = df_engineered['assessment_score']\n",
    "\n",
    "# Statistical feature selection\n",
    "selector = SelectKBest(score_func=f_regression, k=15)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "selected_features = X.columns[selector.get_support()].tolist()\n",
    "\n",
    "# Get feature scores\n",
    "feature_scores = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'score': selector.scores_,\n",
    "    'selected': selector.get_support()\n",
    "}).sort_values('score', ascending=False)\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(14, 8))\n",
    "colors = ['green' if selected else 'lightgray' for selected in feature_scores['selected']]\n",
    "bars = plt.bar(range(len(feature_scores)), feature_scores['score'], color=colors, alpha=0.7)\n",
    "plt.title('Feature Importance Scores (F-regression)\\nGreen bars indicate selected features', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('F-Score')\n",
    "plt.xticks(range(len(feature_scores)), feature_scores['feature'], rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== TOP 15 SELECTED FEATURES ===\")\n",
    "for i, (_, row) in enumerate(feature_scores.head(15).iterrows(), 1):\n",
    "    status = \"‚úì\" if row['selected'] else \"‚úó\"\n",
    "    print(f\"{i:2d}. {status} {row['feature']:<25} (Score: {row['score']:.2f})\")\n",
    "\n",
    "print(f\"\\nSelected {len(selected_features)} features out of {len(feature_cols)} total features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training and Comparison <a id=\"model-training\"></a>\n",
    "\n",
    "Training multiple machine learning algorithms and comparing their performance:\n",
    "- Linear models (Linear Regression, Ridge, Lasso)\n",
    "- Tree-based models (Random Forest, Gradient Boosting)\n",
    "- Support Vector Regression\n",
    "- Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "X = df_engineered[selected_features]\n",
    "y = df_engineered['assessment_score']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=pd.cut(y, bins=5))\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "print(f\"Features used: {len(selected_features)}\")\n",
    "\n",
    "# Define models to compare\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=0.1),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'SVR': SVR(kernel='rbf', C=1.0),\n",
    "    'Neural Network': MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "}\n",
    "\n",
    "print(f\"\\nTraining {len(models)} different models...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate all models\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "print(\"=== MODEL TRAINING AND EVALUATION ===\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "    \n",
    "    results[name] = {\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_mae': train_mae,\n",
    "        'test_mae': test_mae,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'y_pred_test': y_pred_test\n",
    "    }\n",
    "    \n",
    "    print(f\"  Train R¬≤: {train_r2:.4f} | Test R¬≤: {test_r2:.4f}\")\n",
    "    print(f\"  Train RMSE: {train_rmse:.4f} | Test RMSE: {test_rmse:.4f}\")\n",
    "    print(f\"  CV R¬≤ (5-fold): {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "\n",
    "print(\"\\n=== MODEL COMPARISON COMPLETE ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive model comparison visualizations\n",
    "# **Graph Justification**: These plots help compare model performance across multiple metrics\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('Comprehensive Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. R¬≤ Score Comparison\n",
    "model_names = list(results.keys())\n",
    "train_r2_scores = [results[name]['train_r2'] for name in model_names]\n",
    "test_r2_scores = [results[name]['test_r2'] for name in model_names]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 0].bar(x - width/2, train_r2_scores, width, label='Train R¬≤', alpha=0.8, color='skyblue')\n",
    "axes[0, 0].bar(x + width/2, test_r2_scores, width, label='Test R¬≤', alpha=0.8, color='lightcoral')\n",
    "axes[0, 0].set_title('R¬≤ Score Comparison')\n",
    "axes[0, 0].set_ylabel('R¬≤ Score')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. RMSE Comparison\n",
    "train_rmse_scores = [results[name]['train_rmse'] for name in model_names]\n",
    "test_rmse_scores = [results[name]['test_rmse'] for name in model_names]\n",
    "\n",
    "axes[0, 1].bar(x - width/2, train_rmse_scores, width, label='Train RMSE', alpha=0.8, color='lightgreen')\n",
    "axes[0, 1].bar(x + width/2, test_rmse_scores, width, label='Test RMSE', alpha=0.8, color='orange')\n",
    "axes[0, 1].set_title('RMSE Comparison (Lower is Better)')\n",
    "axes[0, 1].set_ylabel('RMSE')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Cross-Validation Scores\n",
    "cv_means = [results[name]['cv_mean'] for name in model_names]\n",
    "cv_stds = [results[name]['cv_std'] for name in model_names]\n",
    "\n",
    "axes[0, 2].bar(model_names, cv_means, yerr=cv_stds, capsize=5, alpha=0.8, color='purple')\n",
    "axes[0, 2].set_title('Cross-Validation R¬≤ Scores (5-fold)')\n",
    "axes[0, 2].set_ylabel('CV R¬≤ Score')\n",
    "axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Actual vs Predicted for best model\n",
    "best_model_name = max(results.keys(), key=lambda x: results[x]['test_r2'])\n",
    "best_predictions = results[best_model_name]['y_pred_test']\n",
    "\n",
    "axes[1, 0].scatter(y_test, best_predictions, alpha=0.6, color='blue')\n",
    "axes[1, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[1, 0].set_title(f'Actual vs Predicted - {best_model_name}')\n",
    "axes[1, 0].set_xlabel('Actual Assessment Score')\n",
    "axes[1, 0].set_ylabel('Predicted Assessment Score')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Residuals plot for best model\n",
    "residuals = y_test - best_predictions\n",
    "axes[1, 1].scatter(best_predictions, residuals, alpha=0.6, color='green')\n",
    "axes[1, 1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1, 1].set_title(f'Residuals Plot - {best_model_name}')\n",
    "axes[1, 1].set_xlabel('Predicted Assessment Score')\n",
    "axes[1, 1].set_ylabel('Residuals')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Model Performance Summary\n",
    "performance_df = pd.DataFrame(results).T[['test_r2', 'test_rmse', 'cv_mean']]\n",
    "performance_df.plot(kind='bar', ax=axes[1, 2], alpha=0.8)\n",
    "axes[1, 2].set_title('Model Performance Summary')\n",
    "axes[1, 2].set_ylabel('Score')\n",
    "axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "axes[1, 2].legend(['Test R¬≤', 'Test RMSE', 'CV Mean R¬≤'])\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed results table\n",
    "print(\"\\n=== DETAILED MODEL PERFORMANCE RESULTS ===\")\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df[['test_r2', 'test_rmse', 'test_mae', 'cv_mean', 'cv_std']]\n",
    "results_df.columns = ['Test R¬≤', 'Test RMSE', 'Test MAE', 'CV Mean R¬≤', 'CV Std R¬≤']\n",
    "results_df = results_df.round(4)\n",
    "results_df = results_df.sort_values('Test R¬≤', ascending=False)\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"   Test R¬≤: {results[best_model_name]['test_r2']:.4f}\")\n",
    "print(f\"   Test RMSE: {results[best_model_name]['test_rmse']:.4f}\")\n",
    "print(f\"   CV R¬≤: {results[best_model_name]['cv_mean']:.4f} ¬± {results[best_model_name]['cv_std']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation <a id=\"evaluation\"></a>\n",
    "\n",
    "Deep dive into the best performing model with:\n",
    "- Feature importance analysis\n",
    "- Learning curves\n",
    "- Hyperparameter optimization\n",
    "- Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization for the best model\n",
    "print(f\"=== HYPERPARAMETER OPTIMIZATION FOR {best_model_name} ===\")\n",
    "\n",
    "if best_model_name == 'Random Forest':\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    base_model = RandomForestRegressor(random_state=42)\n",
    "    \n",
    "elif best_model_name == 'Gradient Boosting':\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "    base_model = GradientBoostingRegressor(random_state=42)\n",
    "    \n",
    "else:\n",
    "    # Default to Random Forest if other models are best\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "    base_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Performing grid search with 3-fold cross-validation...\")\n",
    "grid_search = GridSearchCV(base_model, param_grid, cv=3, scoring='r2', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best model\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_score = grid_search.best_score_\n",
    "\n",
    "print(f\"\\nBest parameters: {best_params}\")\n",
    "print(f\"Best CV score: {best_cv_score:.4f}\")\n",
    "\n",
    "# Evaluate optimized model\n",
    "y_pred_optimized = best_model.predict(X_test)\n",
    "optimized_r2 = r2_score(y_test, y_pred_optimized)\n",
    "optimized_rmse = np.sqrt(mean_squared_error(y_test, y_pred_optimized))\n",
    "\n",
    "print(f\"\\nOptimized model performance:\")\n",
    "print(f\"Test R¬≤: {optimized_r2:.4f}\")\n",
    "print(f\"Test RMSE: {optimized_rmse:.4f}\")\n",
    "print(f\"Improvement in R¬≤: {optimized_r2 - results[best_model_name]['test_r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis for tree-based models\n",
    "# **Graph Justification**: Feature importance helps understand which factors most influence student performance\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # Get feature importances\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': selected_features,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(data=feature_importance.head(15), x='importance', y='feature', palette='viridis')\n",
    "    plt.title(f'Top 15 Feature Importances - Optimized {best_model_name}', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.ylabel('Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"=== TOP 10 MOST IMPORTANT FEATURES ===\")\n",
    "    for i, (_, row) in enumerate(feature_importance.head(10).iterrows(), 1):\n",
    "        print(f\"{i:2d}. {row['feature']:<25} ({row['importance']:.4f})\")\n",
    "        \n",
    "else:\n",
    "    print(\"Feature importance not available for this model type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves analysis\n",
    "# **Graph Justification**: Learning curves show if the model would benefit from more data or is overfitting\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "print(\"Generating learning curves...\")\n",
    "\n",
    "# Generate learning curves\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_model, X_train, y_train, cv=5, n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), scoring='r2'\n",
    ")\n",
    "\n",
    "# Calculate mean and std\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Score')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "plt.plot(train_sizes, val_mean, 'o-', color='red', label='Validation Score')\n",
    "plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
    "\n",
    "plt.title(f'Learning Curves - Optimized {best_model_name}', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('R¬≤ Score')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis of learning curves\n",
    "final_train_score = train_mean[-1]\n",
    "final_val_score = val_mean[-1]\n",
    "gap = final_train_score - final_val_score\n",
    "\n",
    "print(f\"\\n=== LEARNING CURVE ANALYSIS ===\")\n",
    "print(f\"Final training score: {final_train_score:.4f}\")\n",
    "print(f\"Final validation score: {final_val_score:.4f}\")\n",
    "print(f\"Training-validation gap: {gap:.4f}\")\n",
    "\n",
    "if gap > 0.1:\n",
    "    print(\"‚ö†Ô∏è  Model shows signs of overfitting. Consider regularization or more data.\")\n",
    "elif gap < 0.05:\n",
    "    print(\"‚úÖ Model shows good generalization with minimal overfitting.\")\n",
    "else:\n",
    "    print(\"‚úÖ Model shows acceptable generalization.\")\n",
    "\n",
    "if val_mean[-1] > val_mean[-2]:\n",
    "    print(\"üìà Model performance is still improving with more data.\")\n",
    "else:\n",
    "    print(\"üìä Model performance has plateaued with current data size.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Analysis <a id=\"advanced-analysis\"></a>\n",
    "\n",
    "Advanced techniques for deeper insights:\n",
    "- Student clustering analysis\n",
    "- Principal Component Analysis\n",
    "- Prediction intervals\n",
    "- Model interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student clustering analysis\n",
    "# **Graph Justification**: Clustering helps identify distinct student personas for targeted interventions\n",
    "\n",
    "print(\"=== STUDENT CLUSTERING ANALYSIS ===\")\n",
    "\n",
    "# Prepare data for clustering (use original features, not scaled)\n",
    "clustering_features = ['comprehension', 'attention_span', 'memory_retention', 'problem_solving', 'critical_thinking',\n",
    "                      'study_hours_per_week', 'family_support', 'assessment_score']\n",
    "clustering_data = df[clustering_features]\n",
    "\n",
    "# Determine optimal number of clusters using elbow method\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(clustering_data)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(clustering_data, kmeans.labels_))\n",
    "\n",
    "# Plot elbow curve and silhouette scores\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "ax1.plot(k_range, inertias, 'bo-')\n",
    "ax1.set_title('Elbow Method for Optimal k')\n",
    "ax1.set_xlabel('Number of Clusters (k)')\n",
    "ax1.set_ylabel('Inertia')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(k_range, silhouette_scores, 'ro-')\n",
    "ax2.set_title('Silhouette Score vs Number of Clusters')\n",
    "ax2.set_xlabel('Number of Clusters (k)')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Choose optimal k (highest silhouette score)\n",
    "optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"Optimal number of clusters: {optimal_k} (Silhouette Score: {max(silhouette_scores):.3f})\")\n",
    "\n",
    "# Perform final clustering\n",
    "final_kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "cluster_labels = final_kmeans.fit_predict(clustering_data)\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df_clustered = df.copy()\n",
    "df_clustered['cluster'] = cluster_labels\n",
    "\n",
    "print(f\"\\nCluster distribution:\")\n",
    "cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "for cluster, count in cluster_counts.items():\n",
    "    print(f\"Cluster {cluster}: {count} students ({count/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters and analyze characteristics\n",
    "# **Graph Justification**: Cluster visualization helps understand student personas and their characteristics\n",
    "\n",
    "# PCA for visualization\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca_data = pca.fit_transform(clustering_data)\n",
    "\n",
    "# Create comprehensive cluster analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Student Cluster Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. PCA visualization of clusters\n",
    "scatter = axes[0, 0].scatter(pca_data[:, 0], pca_data[:, 1], c=cluster_labels, cmap='viridis', alpha=0.7)\n",
    "axes[0, 0].set_title('Student Clusters (PCA Visualization)')\n",
    "axes[0, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "axes[0, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "plt.colorbar(scatter, ax=axes[0, 0])\n",
    "\n",
    "# 2. Cluster characteristics heatmap\n",
    "cluster_means = df_clustered.groupby('cluster')[clustering_features].mean()\n",
    "sns.heatmap(cluster_means.T, annot=True, cmap='RdYlBu_r', center=50, ax=axes[0, 1], fmt='.1f')\n",
    "axes[0, 1].set_title('Cluster Characteristics Heatmap')\n",
    "axes[0, 1].set_xlabel('Cluster')\n",
    "\n",
    "# 3. Assessment score distribution by cluster\n",
    "df_clustered.boxplot(column='assessment_score', by='cluster', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Assessment Score Distribution by Cluster')\n",
    "axes[1, 0].set_xlabel('Cluster')\n",
    "axes[1, 0].set_ylabel('Assessment Score')\n",
    "\n",
    "# 4. Study hours vs performance by cluster\n",
    "for cluster in range(optimal_k):\n",
    "    cluster_data = df_clustered[df_clustered['cluster'] == cluster]\n",
    "    axes[1, 1].scatter(cluster_data['study_hours_per_week'], cluster_data['assessment_score'], \n",
    "                      label=f'Cluster {cluster}', alpha=0.7)\n",
    "axes[1, 1].set_title('Study Hours vs Assessment Score by Cluster')\n",
    "axes[1, 1].set_xlabel('Study Hours per Week')\n",
    "axes[1, 1].set_ylabel('Assessment Score')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze cluster characteristics\n",
    "print(\"\\n=== CLUSTER CHARACTERISTICS ANALYSIS ===\")\n",
    "for cluster in range(optimal_k):\n",
    "    cluster_data = df_clustered[df_clustered['cluster'] == cluster]\n",
    "    print(f\"\\nüìä CLUSTER {cluster} ({len(cluster_data)} students):\")\n",
    "    print(f\"   Average Assessment Score: {cluster_data['assessment_score'].mean():.1f}\")\n",
    "    print(f\"   Average Study Hours: {cluster_data['study_hours_per_week'].mean():.1f}\")\n",
    "    print(f\"   Average Comprehension: {cluster_data['comprehension'].mean():.1f}\")\n",
    "    print(f\"   Average Attention Span: {cluster_data['attention_span'].mean():.1f}\")\n",
    "    print(f\"   Average Family Support: {cluster_data['family_support'].mean():.1f}\")\n",
    "    \n",
    "    # Identify cluster persona\n",
    "    avg_score = cluster_data['assessment_score'].mean()\n",
    "    avg_study = cluster_data['study_hours_per_week'].mean()\n",
    "    avg_support = cluster_data['family_support'].mean()\n",
    "    \n",
    "    if avg_score >= 75:\n",
    "        performance = \"High Performers\"\n",
    "    elif avg_score >= 50:\n",
    "        performance = \"Average Performers\"\n",
    "    else:\n",
    "        performance = \"Struggling Students\"\n",
    "    \n",
    "    print(f\"   üè∑Ô∏è  Persona: {performance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model interpretability and prediction intervals\n",
    "# **Graph Justification**: Prediction intervals show model uncertainty, crucial for educational decisions\n",
    "\n",
    "print(\"=== MODEL INTERPRETABILITY ANALYSIS ===\")\n",
    "\n",
    "# Calculate prediction intervals using bootstrap\n",
    "def bootstrap_predictions(model, X, n_bootstrap=100):\n",
    "    \"\"\"\n",
    "    Calculate prediction intervals using bootstrap sampling.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    n_samples = len(X)\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        # Bootstrap sample\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        X_bootstrap = X.iloc[indices]\n",
    "        \n",
    "        # Make predictions\n",
    "        pred = model.predict(X_bootstrap)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # Calculate intervals\n",
    "    lower = np.percentile(predictions, 2.5, axis=0)\n",
    "    upper = np.percentile(predictions, 97.5, axis=0)\n",
    "    mean_pred = np.mean(predictions, axis=0)\n",
    "    \n",
    "    return mean_pred, lower, upper\n",
    "\n",
    "# Calculate prediction intervals for test set\n",
    "print(\"Calculating prediction intervals using bootstrap...\")\n",
    "mean_pred, lower_bound, upper_bound = bootstrap_predictions(best_model, X_test, n_bootstrap=50)\n",
    "\n",
    "# Create prediction interval plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 1. Prediction intervals\n",
    "sorted_indices = np.argsort(y_test)\n",
    "ax1.fill_between(range(len(y_test)), lower_bound[sorted_indices], upper_bound[sorted_indices], \n",
    "                alpha=0.3, color='blue', label='95% Prediction Interval')\n",
    "ax1.plot(range(len(y_test)), y_test.iloc[sorted_indices], 'ro', alpha=0.7, label='Actual', markersize=3)\n",
    "ax1.plot(range(len(y_test)), mean_pred[sorted_indices], 'b-', alpha=0.8, label='Predicted Mean')\n",
    "ax1.set_title('Prediction Intervals (95% Confidence)')\n",
    "ax1.set_xlabel('Test Samples (sorted by actual score)')\n",
    "ax1.set_ylabel('Assessment Score')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Prediction uncertainty vs actual score\n",
    "uncertainty = upper_bound - lower_bound\n",
    "ax2.scatter(y_test, uncertainty, alpha=0.6, color='purple')\n",
    "ax2.set_title('Prediction Uncertainty vs Actual Score')\n",
    "ax2.set_xlabel('Actual Assessment Score')\n",
    "ax2.set_ylabel('Prediction Interval Width')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate coverage and interval statistics\n",
    "coverage = np.mean((y_test >= lower_bound) & (y_test <= upper_bound))\n",
    "avg_interval_width = np.mean(uncertainty)\n",
    "\n",
    "print(f\"\\n=== PREDICTION INTERVAL ANALYSIS ===\")\n",
    "print(f\"Coverage (should be ~95%): {coverage:.1%}\")\n",
    "print(f\"Average interval width: {avg_interval_width:.2f} points\")\n",
    "print(f\"Median interval width: {np.median(uncertainty):.2f} points\")\n",
    "\n",
    "if coverage >= 0.90:\n",
    "    print(\"‚úÖ Good coverage - prediction intervals are reliable\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Low coverage - prediction intervals may be too narrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions and Recommendations <a id=\"conclusions\"></a>\n",
    "\n",
    "Summary of findings and actionable insights for educational stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive analysis report\n",
    "print(\"=\" * 80)\n",
    "print(\"üéì COMPREHENSIVE STUDENT PERFORMANCE ANALYSIS REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìä DATASET OVERVIEW:\")\n",
    "print(f\"   ‚Ä¢ Total students analyzed: {len(df):,}\")\n",
    "print(f\"   ‚Ä¢ Features analyzed: {len(selected_features)}\")\n",
    "print(f\"   ‚Ä¢ Average assessment score: {df['assessment_score'].mean():.1f} ¬± {df['assessment_score'].std():.1f}\")\n",
    "print(f\"   ‚Ä¢ Score range: {df['assessment_score'].min():.1f} - {df['assessment_score'].max():.1f}\")\n",
    "\n",
    "print(f\"\\nü§ñ BEST PERFORMING MODEL:\")\n",
    "print(f\"   ‚Ä¢ Algorithm: {best_model_name}\")\n",
    "print(f\"   ‚Ä¢ Test R¬≤ Score: {optimized_r2:.4f} ({optimized_r2*100:.1f}% variance explained)\")\n",
    "print(f\"   ‚Ä¢ Test RMSE: {optimized_rmse:.2f} points\")\n",
    "print(f\"   ‚Ä¢ Cross-validation R¬≤: {best_cv_score:.4f}\")\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    top_features = feature_importance.head(5)\n",
    "    print(f\"\\nüîç TOP 5 PREDICTIVE FACTORS:\")\n",
    "    for i, (_, row) in enumerate(top_features.iterrows(), 1):\n",
    "        print(f\"   {i}. {row['feature']} (importance: {row['importance']:.3f})\")\n",
    "\n",
    "print(f\"\\nüë• STUDENT PERSONAS IDENTIFIED:\")\n",
    "for cluster in range(optimal_k):\n",
    "    cluster_data = df_clustered[df_clustered['cluster'] == cluster]\n",
    "    avg_score = cluster_data['assessment_score'].mean()\n",
    "    size = len(cluster_data)\n",
    "    percentage = size / len(df) * 100\n",
    "    \n",
    "    if avg_score >= 75:\n",
    "        persona = \"üåü High Achievers\"\n",
    "    elif avg_score >= 50:\n",
    "        persona = \"üìà Steady Performers\"\n",
    "    else:\n",
    "        persona = \"üÜò Need Support\"\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Cluster {cluster}: {persona} - {size} students ({percentage:.1f}%) - Avg Score: {avg_score:.1f}\")\n",
    "\n",
    "print(f\"\\nüìà KEY INSIGHTS:\")\n",
    "\n",
    "# Calculate key correlations\n",
    "key_correlations = {\n",
    "    'Study Hours': df['study_hours_per_week'].corr(df['assessment_score']),\n",
    "    'Family Support': df['family_support'].corr(df['assessment_score']),\n",
    "    'Sleep Hours': df['sleep_hours_per_night'].corr(df['assessment_score']),\n",
    "    'Comprehension': df['comprehension'].corr(df['assessment_score']),\n",
    "    'Problem Solving': df['problem_solving'].corr(df['assessment_score'])\n",
    "}\n",
    "\n",
    "for factor, correlation in sorted(key_correlations.items(), key=lambda x: abs(x[1]), reverse=True):\n",
    "    direction = \"positively\" if correlation > 0 else \"negatively\"\n",
    "    strength = \"strongly\" if abs(correlation) > 0.5 else \"moderately\" if abs(correlation) > 0.3 else \"weakly\"\n",
    "    print(f\"   ‚Ä¢ {factor} is {strength} {direction} correlated with performance (r = {correlation:.3f})\")\n",
    "\n",
    "print(f\"\\nüéØ ACTIONABLE RECOMMENDATIONS:\")\n",
    "\n",
    "# Generate recommendations based on analysis\n",
    "recommendations = []\n",
    "\n",
    "if key_correlations['Family Support'] > 0.3:\n",
    "    recommendations.append(\"Implement family engagement programs to boost student support systems\")\n",
    "\n",
    "if key_correlations['Sleep Hours'] > 0.2:\n",
    "    recommendations.append(\"Educate students about the importance of adequate sleep for academic performance\")\n",
    "\n",
    "if key_correlations['Study Hours'] > 0.2:\n",
    "    recommendations.append(\"Provide study skills training and time management workshops\")\n",
    "\n",
    "# Check for at-risk students\n",
    "at_risk_percentage = len(df[df['assessment_score'] < 40]) / len(df) * 100\n",
    "if at_risk_percentage > 10:\n",
    "    recommendations.append(f\"Develop targeted intervention programs for {at_risk_percentage:.1f}% of students scoring below 40\")\n",
    "\n",
    "# Technology recommendations\n",
    "tech_impact = df.groupby('computer_access')['assessment_score'].mean().diff().iloc[-1]\n",
    "if tech_impact > 5:\n",
    "    recommendations.append(f\"Expand computer access programs (current impact: +{tech_impact:.1f} points)\")\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"   {i}. {rec}\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  MODEL LIMITATIONS:\")\n",
    "print(f\"   ‚Ä¢ Prediction intervals average ¬±{avg_interval_width:.1f} points (95% confidence)\")\n",
    "print(f\"   ‚Ä¢ Model explains {optimized_r2*100:.1f}% of performance variance\")\n",
    "print(f\"   ‚Ä¢ {(1-optimized_r2)*100:.1f}% of variance due to unmeasured factors\")\n",
    "print(f\"   ‚Ä¢ Results based on synthetic data - validate with real student data\")\n",
    "\n",
    "print(f\"\\nüîÑ NEXT STEPS:\")\n",
    "print(f\"   1. Validate model with real student performance data\")\n",
    "print(f\"   2. Implement continuous monitoring and model updates\")\n",
    "print(f\"   3. Develop automated early warning system for at-risk students\")\n",
    "print(f\"   4. Create personalized intervention recommendations\")\n",
    "print(f\"   5. Establish feedback loop to improve model accuracy\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìã ANALYSIS COMPLETE - Ready for Educational Implementation\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and results for deployment\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "print(\"=== SAVING MODEL AND RESULTS ===\")\n",
    "\n",
    "# Save the optimized model\n",
    "joblib.dump(best_model, 'optimized_student_performance_model.pkl')\n",
    "print(\"‚úÖ Saved optimized model to 'optimized_student_performance_model.pkl'\")\n",
    "\n",
    "# Save preprocessing components\n",
    "joblib.dump(scalers, 'preprocessing_scalers.pkl')\n",
    "joblib.dump(encoders, 'preprocessing_encoders.pkl')\n",
    "print(\"‚úÖ Saved preprocessing components\")\n",
    "\n",
    "# Save feature selection\n",
    "with open('selected_features.json', 'w') as f:\n",
    "    json.dump(selected_features, f)\n",
    "print(\"‚úÖ Saved selected features list\")\n",
    "\n",
    "# Save analysis results\n",
    "analysis_results = {\n",
    "    'model_performance': {\n",
    "        'best_model': best_model_name,\n",
    "        'test_r2': float(optimized_r2),\n",
    "        'test_rmse': float(optimized_rmse),\n",
    "        'cv_score': float(best_cv_score)\n",
    "    },\n",
    "    'feature_importance': feature_importance.to_dict('records') if hasattr(best_model, 'feature_importances_') else None,\n",
    "    'cluster_analysis': {\n",
    "        'optimal_clusters': int(optimal_k),\n",
    "        'cluster_characteristics': cluster_means.to_dict()\n",
    "    },\n",
    "    'key_correlations': key_correlations,\n",
    "    'recommendations': recommendations\n",
    "}\n",
    "\n",
    "with open('analysis_results.json', 'w') as f:\n",
    "    json.dump(analysis_results, f, indent=2)\n",
    "print(\"‚úÖ Saved comprehensive analysis results\")\n",
    "\n",
    "print(\"\\nüéâ All components saved successfully!\")\n",
    "print(\"üìÅ Files created:\")\n",
    "print(\"   ‚Ä¢ optimized_student_performance_model.pkl\")\n",
    "print(\"   ‚Ä¢ preprocessing_scalers.pkl\")\n",
    "print(\"   ‚Ä¢ preprocessing_encoders.pkl\")\n",
    "print(\"   ‚Ä¢ selected_features.json\")\n",
    "print(\"   ‚Ä¢ analysis_results.json\")\n",
    "\n",
    "print(\"\\nüöÄ Ready for production deployment!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
